{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Code:\n",
    "\n",
    "## This is a brief synopsis of some go-to Data Science code\n",
    "\n",
    "It's a work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pretty simple, but I like it as the first thing I run\n",
    "def eda(dataframe):\n",
    "    print(\"missing values \\n\", dataframe.isnull().sum(), '\\n')\n",
    "    print(\"dataframe index \\n\", dataframe.index, '\\n')\n",
    "    print(\"dataframe types \\n\", dataframe.dtypes, '\\n')\n",
    "    print(\"dataframe shape \\n\", dataframe.shape, '\\n')\n",
    "    print(\"dataframe describe \\n\", dataframe.describe(include='all'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of changing dtypes of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], infer_datetime_format=True)\n",
    "# Fix date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Year, Month, Day, etc columns if wanted\n",
    "df[\"Year\"] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"Category Name\"] = df[\"Category Name\"].astype('category')\n",
    "df[\"Vendor Number\"] = df[\"Vendor Number\"].astype('category')\n",
    "df[\"Item Number\"] = df[\"Item Number\"].astype('category')\n",
    "df[\"Item Description\"] = df[\"Item Description\"].astype('category')\n",
    "# Change category columns to dtype category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"State Bottle Retail\"] = df[\"State Bottle Retail\"].apply(lambda x: x.strip('$')).astype(float)\n",
    "df[\"Sale (Dollars)\"] = df[\"Sale (Dollars)\"].apply(lambda x: x.strip('$')).astype(float)\n",
    "# Change dollar amounts to floats for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Default seaborn settings are pretty\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "# This makes your plots bigger\n",
    "sns.set(font_scale=1.5)\n",
    "# This increases the size of the label fonts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Bar chart in seaborn:\n",
    "\n",
    "the 'hue=' attribute adds a column to split the count on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Species', hue='WnvPresent', data=train)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple scatter plot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Only actually necessary if you want to save an image of the file\n",
    "sns.set(font_scale=2)\n",
    "# set font size of labels\n",
    "sns.regplot('Age', 'Median', data=dataframe, fit_reg=False, scatter_kws={\"s\": 200})\n",
    "# plot first data points, takes x column, y column, DataFrame, can fit a linear regression line, can change size of points\n",
    "sns.regplot('Age', '90th Percentile', data=y, fit_reg=False, scatter_kws={\"s\": 200})\n",
    "# Second plot for fun\n",
    "plt.title('Median comments over time')\n",
    "# Title\n",
    "plt.xlabel('Age of Post in Hours')\n",
    "# X axis label\n",
    "plt.ylabel('Number of Comments')\n",
    "# Y axis label\n",
    "plt.legend(['Median', '90th Percentile'])\n",
    "# Legend if you do use both plots\n",
    "fig.savefig('/Users/Dale/Desktop/RedMed.svg', format='svg', dpi=2000)\n",
    "# Saves the file here if you want, comment out otherwise\n",
    "plt.show()\n",
    "# Show it (though unnecessary with matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget your train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(dataframe, test_size=0.3, random_state=42)\n",
    "# You can pass a dataframe with both your X and y, good if you want to use Patsy format\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=42)\n",
    "# Pass your X features seperate and then your y target\n",
    "# I prefer this split as you don't have to drop things later and can do transformations easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn Evaluation:\n",
    "It expects you to have already used .predict().  The second two parameters are only if you want to get the Roc-Auc score and only work if you have bianary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_auc_score\n",
    "\n",
    "def eval_sklearn_model(y_true, predictions, model=None, X=None):\n",
    "    \"\"\"This function takes the true values for y and the predictions made by the model and prints out the confusion matrix along with Accuracy, Precision, and, if model and X provided, Roc_Auc Scores.\"\"\"\n",
    "    cnf_matrix = confusion_matrix(y_true, predictions)\n",
    "\n",
    "    print('True Negative: ', cnf_matrix[0, 0], '| False Positive: ', cnf_matrix[0, 1])\n",
    "    print('False Negative: ', cnf_matrix[1, 0], '| True Positive: ', cnf_matrix[1, 1], '\\n')\n",
    "\n",
    "    sensitivity = cnf_matrix[1, 1]/ (cnf_matrix[1, 0] + cnf_matrix[1, 1])\n",
    "    specificity = cnf_matrix[0, 0]/ (cnf_matrix[0, 1] + cnf_matrix[0, 0])\n",
    "\n",
    "    print('Sensitivity (TP/ TP + FN): ', sensitivity)\n",
    "    print('Specificity (TN/ TN + FP): ', specificity, '\\n')\n",
    "\n",
    "    print('Accuracy: ', accuracy_score(y_true, predictions, normalize=True))\n",
    "    print('Precision: ', precision_score(y_true, predictions))\n",
    "    if model != None:\n",
    "        print('Roc-Auc: ', roc_auc_score(y_true, [x[1] for x in model.predict_proba(X)]))\n",
    "    else:\n",
    "        pass\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats Models\n",
    "Logistic Regression is good here because of the .summary() feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = sm.logit(\"target_y ~ var_x1 + var_x2 + var_x3 * var_x4\",data = df_train).fit()\n",
    "# You can use the Patsy format here.  var_x3 and var_x4 are 'interation' variables in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate logistic regression in this format:\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('If var_x1 increases by 1, it is ', math.exp(3.2923), ' times as likely the target class will be the case.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic process:\n",
    "    \n",
    "    1) initiate your model\n",
    "    2) cross-validate to get base accuracy\n",
    "    3) if looking for other metrics to compare:\n",
    "        a) use .fit(X_train, y_train)\n",
    "        b) use .predict(X_train) to make preditions of TRAINING data\n",
    "        c) compare these with y_train (can use eval_sklearn_model(y_train, predictions))\n",
    "    4) once you are satisfied with your model, use test data\n",
    "        a) use .fit(X_test)\n",
    "        b) compare these with y_test (can use eval_sklearn_model(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "# Perform cross validation to estimate model performance\n",
    "print(cross_val_score(knn, X_train, y_train, cv=3, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If concerned with more than accuracy (such as sensitivity or specificity, do this as well)\n",
    "# Skip otherwise\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_train)\n",
    "eval_sklearn_model(y_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test on your reserved information\n",
    "knn.fit(X_train, y_train)\n",
    "# Can skip .fit() if performed above\n",
    "test_predictions = knn.predict(X_test)\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train, y_train)\n",
    "test_predictions = log.predict(X_test)\n",
    "print('TD-IDF, Subs, Sensitivity, and Features Logistic Regression TEST SCORE:\\n')\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mnb = MultinomialNB(class_prior=[.9,.1])\n",
    "# use class_prior if you know your classes are unbalanced.  This example I have 90% 0's and 10% 1's\n",
    "mnb.fit(X_train, y_train)\n",
    "test_predictions = mnb.predict(X_test)\n",
    "print('Multinomial Naive Bayes TEST SCORE:\\n')\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', min_samples_leaf=5)\n",
    "dt.fit(X_train, y_train)\n",
    "test_predictions = dt.predict(X_test)\n",
    "print('Decision Tree TEST SCORE:\\n')\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dt = RandomForestClassifier(class_weight='balanced', max_features=750, min_samples_leaf=5, n_estimators=1000, n_jobs=-1)\n",
    "dt.fit(X_train, y_train)\n",
    "test_predictions = dt.predict(X_test)\n",
    "print('Random Forest TEST SCORE:\\n')\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dt = ExtraTreesClassifier(class_weight='balanced', max_features=500, min_samples_leaf=5, n_estimators=100, n_jobs=-1)\n",
    "dt.fit(X_train, y_train)\n",
    "test_predictions = dt.predict(X_test)\n",
    "print('Extra Random Forest TEST SCORE:\\n')\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(scale_pos_weight=(27543/3111), objective='binary:logistic')\n",
    "# make sure to pick the correct objective for the problem\n",
    "# scale_pos_weight is supposed to help with unbalanced classes; it recommended number of negative cases divided by positive\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = xgb.predict(X_test)\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an example.\n",
    "It is expecting a X_train, y_train, X_test, y_test split to have been performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Pick which estimators you want to test (example is for random forest)\n",
    "param_grid = dict(n_estimators = [100],\n",
    "                 max_features = [250, 500, 750, 1000, 2000, 5000],\n",
    "                 min_samples_leaf = [5, 15, 50, 100],\n",
    "                 )\n",
    "\n",
    "# Switch out the model here that you would like to test\n",
    "model = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc')\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_results = {'params': list(grid.best_params_.items()), 'score': grid.best_score_}\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best_model = best_model.fit(X_train, y_train)\n",
    "\n",
    "score = best_model.score(X_test, y_test)\n",
    "\n",
    "print('Number of Models Run: ', len(n_estimators) * len(max_features) * len(min_samples_leaf) * 3)\n",
    "# YOU MUST UPDATE THIS LINE TO HAVE SAME ESTIMATORS AS IN DICTIONARY\n",
    "print(\"{} Score: {:0.3}\".format('Decision Tree Classifier', score.mean().round(3)), '\\n')\n",
    "print('Elapsed Time: {:0.3}'.format( time.time() - start_time), ' seconds', '\\n')\n",
    "print(grid.best_estimator_, '\\n')\n",
    "print('Best Hyperparameters we tested for', '\\n', best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your best model from the grid is already fit and saved as best_model\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print('Grid Search TEST SCORE:\\n')\n",
    "# function created above should be run before this cell\n",
    "eval_sklearn_model(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bokeh Google Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import (\n",
    "  GMapPlot, GMapOptions, ColumnDataSource, Circle, DataRange1d, PanTool, WheelZoomTool, BoxSelectTool\n",
    ")\n",
    "\n",
    "map_options = GMapOptions(lat=41.87, lng=-87.70, map_type=\"hybrid\", zoom=10)\n",
    "\n",
    "plot = GMapPlot(\n",
    "    x_range=DataRange1d(), y_range=DataRange1d(), map_options=map_options\n",
    ")\n",
    "plot.title.text = \"Chicago\"\n",
    "\n",
    "# For GMaps to function, Google requires you obtain and enable an API key:\n",
    "#\n",
    "#     https://developers.google.com/maps/documentation/javascript/get-api-key\n",
    "#\n",
    "# Replace the value below with your personal API key:\n",
    "plot.api_key = \"AIzaSyACt4u00p7Z7djOutZHXa7JshUOA0m2Cjw\"\n",
    "\n",
    "source = ColumnDataSource(\n",
    "    data=dict(\n",
    "        lat=[41.786, 41.995,],\n",
    "        lon=[-87.752, -87.933,],\n",
    "    )\n",
    ")\n",
    "\n",
    "circle = Circle(x=\"lon\", y=\"lat\", size=15, fill_color=\"blue\", fill_alpha=0.8, line_color=None)\n",
    "plot.add_glyph(source, circle)\n",
    "\n",
    "plot.add_tools(PanTool(), WheelZoomTool(), BoxSelectTool())\n",
    "output_file(\"gmap_plot.html\")\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
